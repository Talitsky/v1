---
layout: post
title: "A quick start guide"
author: "Aleksandr Talitckii"
categories: facts
tags: [facts,documentation]
image: cuba-1.jpg
---

## What is Kernel Learning?

Kernel methods for classification and regression (and Support Vector Machines (SVMs) in particular) require selection of a kernel. Kernel Learning (KL) algorithms automate this task by finding the kernel , *k* from the set *K* which optimizes an achievable metric such as the soft margin (for classification).

To understand how the choice of *K* influences performance and robustness, three properties were proposed to characterize the set *K* - tractability, density, and universality. 
* *K* is tractable if *K* is convex (or, preferably, a linear variety) - implying the KL problem is solvable using.
* The set *K* has the density property if, for any *e>0* and any positive kernel, *k* there exists a *k* from *K* such that:
$$
\|k - k\| \le e
$$
* *K* has the universal property if any *k* from *K* is universal - ensuring the classifier/predictor will perform arbitrarily well on large sets of training data.
More about this property you can find [here](https://arxiv.org/pdf/2106.08443.pdf)

The Tessellated Kernels (TKs) were shown to have all 3 properties

## Tessellated Kernels
TK can be represented in the following way:
$$
{\mcl K} \hspace{-1mm} := \hspace{-1mm} \left\{ k ~|~ k(x,y)= \hspace{-2mm} \int_{X} \hspace{-2mm} N(z,x)^T P N(z,y) dz, P \geq 0 \right\} \label{eqn:tractable} 
$$
where 
$$
N^d_{T}(z,x) = \bmat{Z_d(z,x)\mathbf I(z-x) \\ Z_d(z,x)\mathbf I(x-z) } \;\;\;\;  \mathbf I(z) = \begin{cases}
    1       & \quad z \ge 0\\
    0  & \quad \text{otherwise,}\\
\end{cases}
$$
